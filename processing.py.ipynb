{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is for windows but first you have to download wget from this \n",
    "#link(https://eternallybored.org/misc/wget/) and install. for installation pls follow the \n",
    "#link video (https://www.jcchouinard.com/wget/)\n",
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip ml-100k.zip\n",
    "\n",
    "import pandas as pd\n",
    "#if you confused about the seprator so just copy the symbol from the csv \n",
    "#file and past it here.\n",
    "data = pd.read_csv('D:/ml-100k/ua.base',sep=\"\t\")#kindly use your own path\n",
    "data[:5]\n",
    "\n",
    "#As the dataset is ordered by user ID, we shuffle it as a precaution. Then, we take\n",
    "#a look at the first few lines:\n",
    "#%cd ml-100k\n",
    "#!shuf ua.base -o ua.base.shuffled\n",
    "#!head -5 ua.base.shuffled\n",
    "\n",
    "#for shuffling use this code\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "\n",
    "#We define sizing constants:\n",
    "num_users = 943\n",
    "num_movies = 1682\n",
    "num_features = num_users+num_movies\n",
    "num_ratings_train = 90570\n",
    "num_ratings_test = 9430\n",
    "\n",
    "#Now, let's write a function to load a dataset into a sparse matrix. Based on the\n",
    "#previous explanation, we go through the dataset line by line. In the X matrix,\n",
    "#we set the appropriate user and movie columns to 1. We also store the rating in the\n",
    "#Y vector:\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "def loadDataset(filename, lines, columns):\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(num_users)+int(movieId)-1] = 1\n",
    "            Y.append(int(rating))\n",
    "            line=line+1\n",
    "    Y=np.array(Y).astype('float32')\n",
    "    return X,Y\n",
    "\n",
    "#We then process the training and test datasets.\n",
    "X_train, Y_train = loadDataset('D:/ml-100k/ua.base',num_ratings_train,num_features)\n",
    "X_test, Y_test = loadDataset('D:/ml-100k/ua.test',num_ratings_test,num_features)\n",
    "\n",
    "#We check that the shapes are what we expect\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#Now, let's write a function that converts a dataset to the RecordIO-wrapped\n",
    "#protobuf, and uploads it to an S3 bucket. We first create an in-memory binary\n",
    "#stream with io.BytesIO(). Then, we use the life-saving write_spmatrix_\n",
    "#to_sparse_tensor() function to write the sample matrix and the label vector to\n",
    "#that buffer in protobuf format. Finally, we use boto3 to upload the buffer to S3:\n",
    "import io, boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "\n",
    "#Had our data been stored in a numpy array instead of lilmatrix, we would\n",
    "#have used the write_numpy_to_dense_tensor() function instead. It has the\n",
    "#same effect.\n",
    "#We apply this function to both datasets, and we store their S3 paths\n",
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'fm-movielens'\n",
    "train_key = 'train.protobuf'\n",
    "train_prefix = '{}/{}'.format(prefix, 'train')\n",
    "test_key = 'test.protobuf'\n",
    "test_prefix = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix = 's3://{}/{}/output'.format(bucket,prefix)\n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train,bucket, train_prefix, train_key)\n",
    "test_data = writeDatasetToProtobuf(X_test, Y_test,bucket, test_prefix, test_key)\n",
    "\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "def resolve_sm_role():\n",
    "    client = boto3.client('iam', region_name=region)\n",
    "    response_roles = client.list_roles(PathPrefix='/',\n",
    "                                       # Marker='string'\n",
    "                                       MaxItems=999) \n",
    "    for role in response_roles['Roles']:\n",
    "        if role['RoleName'].startswith('AmazonSageMaker-ExecutionRole-'):\n",
    "               # print('Resolved SageMaker IAM Role to: ' + str(role))\n",
    "                return role['Arn']\n",
    "    raise Exception('Could not resolve what should be the SageMaker role to be used')\n",
    "                        #resolve_sm_role()\n",
    "                        #role = get_execution_role()\n",
    "role = resolve_sm_role()\n",
    "role\n",
    "\n",
    "#What comes next is SageMaker business as usual. We find the name of the\n",
    "#Factorization Machines container, configure the Estimator function, and set the\n",
    "#hyperparameters:\n",
    "from sagemaker import image_uris\n",
    "region=boto3.Session().region_name\n",
    "container=image_uris.retrieve('factorization-machines',\n",
    "region)\n",
    "fm=sagemaker.estimator.Estimator(\n",
    "container,\n",
    "role=role,\n",
    "instance_count=1,\n",
    "instance_type='ml.m4.xlarge',#you can use ml.m5.xlarge both for free tier.\n",
    "output_path=output_prefix)\n",
    "fm.set_hyperparameters(\n",
    "feature_dim=num_features,\n",
    "predictor_type='regressor',\n",
    "num_factors=64,\n",
    "epochs=10)\n",
    "\n",
    "#We then launch the training job. Did you notice that we didn't configure training\n",
    "#inputs? We're simply passing the location of the two protobuf files. As protobuf\n",
    "#is the default format for Factorization Machines (as well as other built-in\n",
    "#algorithms), we can save a step:\n",
    "fm.fit({'train': train_data, 'test': test_data})\n",
    "\n",
    "#We'll now send samples to the endpoint in JSON format (https://docs.aws.\n",
    "#amazon.com/sagemaker/latest/dg/fact-machines.html#fminputoutput).\n",
    "#For this purpose, we write a custom serializer to convert input\n",
    "#data to JSON. The default JSON deserializer will be used automatically since we set\n",
    "#the content type to 'application/json':\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "endpoint_name = 'fm-movielens-100k'\n",
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {'instances': []}\n",
    "        for row in data:\n",
    "            js['instances'].append({'features': row.tolist()})\n",
    "            return json.dumps(js)\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",#you can use ml.m5.xlarge both for free tier.\n",
    "    serializer=FMSerializer(),\n",
    "    deserializer= JSONDeserializer()\n",
    ")\n",
    "\n",
    "#We send the first three samples of the test set for prediction\n",
    "result = fm_predictor.predict(X_test[:3].toarray())\n",
    "print(result)\n",
    "\n",
    "#Finally, we delete the endpoint.\n",
    "fm_predictor.delete_endpoint()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
